# -*- coding: utf-8 -*-
"""teste unitario das 5 IA tcc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y-4Tq-2-qH0nw6pDGijTX4rleDMC4jnd
"""

# C√âLULA 1: SETUP E IMPORTA√á√ïES (TODOS OS 4 MODELOS)

# Instala√ß√£o de Depend√™ncias (Execute apenas se o ambiente for Google Colab/Jupyter)
!pip install pandas numpy scikit-learn tensorflow matplotlib
print("‚úÖ Bibliotecas essenciais (pandas, numpy, scikit-learn, tensorflow, matplotlib) instaladas com sucesso!")

# Imports Essenciais
import pandas as pd
import numpy as np
import random
import warnings
import gc
import os
from time import time
# Imports de Ambiente (Ajuste ou remova conforme seu ambiente)
from google.colab import files # Necess√°rio para o Colab

# Imports de Machine Learning (Scikit-learn)
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import MinMaxScaler # Necess√°rio para LSTM

# Imports de Deep Learning (TensorFlow/Keras)
try:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense
    from tensorflow.keras.optimizers import Adam
except ImportError:
    print("‚ö†Ô∏è Aviso: TensorFlow/Keras n√£o encontrado. O modelo LSTM n√£o funcionar√°. Instale com: pip install tensorflow")
    # Garante que as demais c√©lulas n√£o falhem com NameError
    class Sequential: pass
    class LSTM: pass
    class Dense: pass
    class Adam: pass
    class MinMaxScaler: pass # Mocka para evitar falha no c√≥digo de imputa√ß√£o

# Imports de Visualiza√ß√£o
import matplotlib.pyplot as plt

# Configura√ß√£o
warnings.filterwarnings("ignore", category=FutureWarning)
print("‚úÖ M√≥dulos importados. Ambiente configurado para 4 metodologias.")

# C√âLULA 2: CARREGAMENTO E PR√â-PROCESSAMENTO (ESTRAT√âGIA ROBUSTA DO SARIMAX)

# --- VARI√ÅVEIS DE CARREGAMENTO ---
file_name = 'dados_A632_H_2017-03-21_2024-12-31.csv'
coluna_data_keyword = 'Data Medicao'
coluna_hora_keyword = 'Hora Medicao'

# Mapeamento para os nomes finais (valores) e as palavras-chave (chaves) para encontrar a coluna original
FINAL_COLUMNS_MAPPING = {
    'PRECIPITACAO': 'PRECIPITACAO_HORARIOMM',
    'TEMPERATURA': 'TEMPERATURA_AR_HORARIA',
    'UMIDADE': 'UMIDADE_AR_HORARIA',
    'PRESSAO': 'PRESSAO_ESTACAO_HORARIA',
}

def find_original_column_robust(df_cols, keyword):
    """Encontra o nome real da coluna na lista com base em uma palavra-chave."""
    keyword_clean = keyword.upper().replace(' ', '').replace('_', '').replace('√ÉO', 'CAO').replace('√Å', 'A').replace('√É', 'A')
    for col in df_cols:
        col_clean = col.upper().replace(' ', '').replace('_', '').replace('(', '').replace(')', '').replace('√Å', 'A').replace('√É', 'A').replace('√á√ÉO', 'CAO')
        if keyword_clean in col_clean:
            return col
    return None

# --- L√≥gica de Upload Robusta (Para resolver o FileNotFoundError) ---
df_global = pd.DataFrame()
if not os.path.exists(file_name) and 'google.colab' in str(get_ipython()):
    print(f"‚ùå Arquivo '{file_name}' n√£o encontrado no diret√≥rio. Por favor, fa√ßa o upload.")
    try:
        uploaded = files.upload()
        if file_name not in uploaded:
             print(f"‚ùå Erro de Upload: O arquivo carregado n√£o tem o nome esperado. Verifique o nome do arquivo.")
             # Tenta carregar o primeiro arquivo enviado, se for o caso
             if uploaded: file_name = next(iter(uploaded))
    except Exception as e:
        print(f"Erro durante o upload: {e}")


# 1. Carregamento do CSV
print("\nIniciando carregamento e pr√©-processamento...")
try:
    df_global = pd.read_csv(
        file_name,
        sep=';',
        encoding='latin-1', # Usando latin-1
        decimal=',',
        skiprows=10, # Usando skiprows=10
        low_memory=False
    )

    colunas_originais = df_global.columns.tolist()

    # 2. Encontrar os nomes reais das colunas de Data e Hora
    original_data_col = find_original_column_robust(colunas_originais, coluna_data_keyword)
    original_hora_col = find_original_column_robust(colunas_originais, coluna_hora_keyword)

    if not original_data_col or not original_hora_col:
        raise KeyError(f"Colunas de Data ou Hora n√£o encontradas! Encontradas: {original_data_col}, {original_hora_col}")

    # 3. Cria√ß√£o do √çndice de Tempo
    print(f"‚úÖ Colunas Data/Hora encontradas: '{original_data_col}' e '{original_hora_col}'")

    # Trata a coluna Hora Medicao (ex: 100 -> 0100)
    df_global[original_hora_col] = df_global[original_hora_col].astype(str).str.zfill(4)

    # Concatena e cria o √≠ndice
    data_hora_string = df_global[original_data_col].astype(str) + ' ' + df_global[original_hora_col].astype(str)
    df_global['DATA_HORA'] = pd.to_datetime(data_hora_string, format='%d/%m/%Y %H%M', errors='coerce')

    # Se a primeira tentativa falhar (muitos NaT), tenta o formato ISO
    if df_global['DATA_HORA'].isna().sum() > len(df_global) * 0.5:
        print("   ‚ö†Ô∏è Formato '%d/%m/%Y %H%M' falhou. Tentando '%Y-%m-%d %H%M'...")
        df_global['DATA_HORA'] = pd.to_datetime(data_hora_string, format='%Y-%m-%d %H%M', errors='coerce')


    df_global.set_index('DATA_HORA', inplace=True)
    df_global.sort_index(inplace=True)

    # Remove as colunas originais de Data e Hora
    df_global.drop(columns=[original_data_col, original_hora_col], inplace=True)

    # 4. Mapeamento das Colunas de Interesse (reutilizando a fun√ß√£o robusta)
    rename_map = {}
    print("\nLocalizando e renomeando colunas de features...")
    for keyword, novo_nome in FINAL_COLUMNS_MAPPING.items():
        original_col = find_original_column_robust(df_global.columns, keyword)
        if original_col:
            rename_map[original_col] = novo_nome
            print(f"  -> '{keyword}' mapeado de '{original_col}' para '{novo_nome}'")
        else:
            print(f"  ‚ùå Aviso Cr√≠tico: Coluna com keyword '{keyword}' n√£o encontrada.")


    if rename_map:
        df_global.rename(columns=rename_map, inplace=True)

    # 5. For√ßa a convers√£o para num√©rico e Sele√ß√£o Final
    COLUNAS_FINAIS = list(FINAL_COLUMNS_MAPPING.values())

    for col in COLUNAS_FINAIS:
        if col not in df_global.columns:
            # Ponto de falha final para debug
            print(f"DEBUG: Colunas atuais do DataFrame: {df_global.columns.tolist()}")
            raise KeyError(f"Coluna final esperada '{col}' n√£o est√° presente no DataFrame.")

        df_global[col] = pd.to_numeric(df_global[col], errors='coerce')

    df_global = df_global[COLUNAS_FINAIS].copy()
    df_global.dropna(how='all', inplace=True)

    print("\n‚úÖ DataFrame carregado, padronizado e pronto.")
    print(f"Tamanho do DataFrame: {df_global.shape}")
    print(df_global.head())

except Exception as e:
    print(f"‚ùå Erro cr√≠tico no carregamento ou pr√©-processamento: {e}")
    print("\nSUGEST√ÉO: Se o erro persistir, o problema pode estar no delimitador (',' em vez de ';') ou na codifica√ß√£o.")
    df_global = pd.DataFrame()

# C√âLULA 3: FUN√á√ïES AUXILIARES (Lags, M√©tricas, Modelos)
# --------------------------------------------------------------------------------

import pandas as pd
import numpy as np
import warnings
import gc
from sklearn.preprocessing import MinMaxScaler
# AVISO: Se voc√™ estiver executando no Colab e 'tensorflow' n√£o estiver carregado
# certifique-se de que a c√©lula de imports essenciais foi executada antes.
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from tensorflow.keras.callbacks import EarlyStopping

# Vari√°vel global essencial para a C√©lula 4
LAG_ORDER = 3

# Fun√ß√µes auxiliares (re-definidas para o ambiente)
def cria_dataset_lags(serie, lag=LAG_ORDER):
    """Cria o dataset de features (lags) e o target (target) para um modelo de regress√£o."""
    X = pd.DataFrame({f'lag_{i}': serie.shift(i) for i in range(1, lag + 1)})
    y = serie.copy()

    df_lags = pd.concat([X, y.rename('target')], axis=1).dropna()

    X_clean = df_lags.drop(columns=['target'])
    y_clean = df_lags['target']

    return X_clean, y_clean

def calcula_metricas(real, pred):
    """Calcula MAE, MAPE, MAD e RMSE."""
    warnings.filterwarnings('ignore', category=RuntimeWarning)
    real = np.array(real)
    pred = np.array(pred)

    val_idx = ~np.isnan(real) & ~np.isnan(pred)
    real = real[val_idx]
    pred = pred[val_idx]

    if len(real) == 0:
        return {'MAE': np.nan, 'MAPE': np.nan, 'MAD': np.nan, 'RMSE': np.nan}

    mae = np.mean(np.abs(real - pred))
    mad = np.mean(np.abs(real - np.mean(real)))
    rmse = np.sqrt(np.mean((real - pred)**2))

    with np.errstate(divide='ignore', invalid='ignore'):
        epsilon = 1e-8
        mape = np.mean(np.abs((real - pred) / (real + epsilon))) * 100
        if np.isnan(mape) or np.isinf(mape) or mape > 1e7:
            mape = np.nan

    warnings.filterwarnings('default', category=RuntimeWarning)
    return {'MAE': mae, 'MAPE': mape, 'MAD': mad, 'RMSE': rmse}

def executa_modelo(nome_metodo, modelo, X_train, y_train, X_test, y_test):
    """Treina e testa o modelo e retorna as m√©tricas e as previs√µes."""
    metricas = {'MAE': np.nan, 'MAPE': np.nan, 'MAD': np.nan, 'RMSE': np.nan, 'M√©todo': nome_metodo}
    pred = None # Inicializa pred como None

    try:
        if 'LSTM' not in nome_metodo:
            modelo_instance = modelo.__class__(**modelo.get_params())
        else:
            modelo_instance = modelo

        if 'LSTM' in nome_metodo:
            scaler = MinMaxScaler(feature_range=(0, 1))
            X_train_scaled = scaler.fit_transform(X_train)
            if X_test.empty:
                print(f"      ‚ö†Ô∏è X_test vazio para {nome_metodo}. Pulando.")
                return metricas, None

            X_test_scaled = scaler.transform(X_test)
            X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
            X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)

            es = EarlyStopping(monitor='loss', patience=3, verbose=0)

            modelo_instance.fit(X_train_reshaped, y_train.values, epochs=50, batch_size=32, verbose=0, callbacks=[es])

            pred_scaled = modelo_instance.predict(X_test_reshaped, verbose=0).flatten()
            pred = pred_scaled * (y_train.max() - y_train.min()) + y_train.min()

        else:
            if X_test.empty:
                print(f"      ‚ö†Ô∏è X_test vazio para {nome_metodo}. Pulando.")
                return metricas, None

            modelo_instance.fit(X_train, y_train)
            pred = modelo_instance.predict(X_test)

        # Atualiza m√©tricas (metricas √© um dicion√°rio)
        metricas.update(calcula_metricas(y_test.values, pred))
        print(f"      ‚úÖ OK. MAE: {metricas['MAE']:.4f} (Teste size: {len(y_test)})")
        return metricas, pred # Retorna um dicion√°rio e um array

    except Exception as e:
        print(f"      ‚ùå Erro ao executar {nome_metodo}: {e}")
        if 'LSTM' in nome_metodo:
            # Tenta liberar mem√≥ria
            del modelo_instance
            gc.collect()
        return metricas, None # Retorna o dicion√°rio de m√©tricas base e None para previs√£o

# 1. Pr√©-compila√ß√£o do LSTM
lstm_model = Sequential([LSTM(50, input_shape=(LAG_ORDER, 1)), Dense(1)])
lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')

# 2. Dicion√°rio final
TODOS_METODOS = {
    'Random Forest': RandomForestRegressor(random_state=42, n_estimators=100),
    'KNN Regressor': KNeighborsRegressor(n_neighbors=5),
    'GBR Regressor': GradientBoostingRegressor(random_state=42),
    'LSTM Regressor': lstm_model
}

# 3. Zera o resultado global (CR√çTICO para re-execu√ß√£o)
globals()['resultados_brutos'] = []
# Assume que df_global e janelas_validacao est√£o definidos nas c√©lulas 2 e 4, respectivamente.
# Apenas para evitar NameError se n√£o estiverem.
globals()['df_global'] = globals().get('df_global', None)

print("‚úÖ Fun√ß√µes auxiliares, modelos (LSTM pr√©-compilado) e 'resultados_brutos' (RESETADO) definidos.")

# C√âLULA 4: PROCESSAMENTO ROLLING ORIGIN (COMPLETA E CORRIGIDA)
# --------------------------------------------------------------------------------
import pandas as pd
from time import time
import numpy as np
import gc

# Assumindo que as seguintes vari√°veis foram definidas nas C√©lulas 2 e 3:
# df_global, janelas_validacao, TODOS_METODOS, simula_lacunas_seq_3, cria_dataset_lags, LAG_ORDER, executa_modelo

# --- Configura√ß√µes Iniciais (Assumindo que df_global est√° carregado) ---
try:
    if globals().get('df_global') is None:
        raise NameError("df_global n√£o encontrado. Execute a C√©lula 2.")
    if 'janelas_validacao' not in globals():
        # Tentativa de importa√ß√£o se n√£o estiver na mem√≥ria
        from __main__ import janelas_validacao

    colunas_caracteristicas = df_global.columns.tolist()
    globals()['resultados_brutos'] = [] # Garante que est√° zerado antes de iniciar

except Exception as e:
    print(f"‚ùå Erro de inicializa√ß√£o: {e}")
    # Cria vari√°veis dummy para evitar falha do interpretador
    colunas_caracteristicas = []
    janelas_validacao = []


print("Iniciando a avalia√ß√£o dos 4 modelos ML com 5 janelas de 12 meses (Foco no per√≠odo 2017-2022)...")

# --- Loop principal para CARACTER√çSTICAS ---
for caracteristica_focal in colunas_caracteristicas:

    print(f"\n################################################################################")
    print(f"PROCESSANDO CARACTER√çSTICA: {caracteristica_focal}")
    print(f"################################################################################")

    series_completa = df_global[caracteristica_focal].copy()

    # --- Loop para JANELAS de Valida√ß√£o (Rolling Origin) ---
    for i, janela in enumerate(janelas_validacao):

        JANELA_FOCAL = janela['nome']

        # 1. Separa√ß√£o dos dados
        series_treino = series_completa.loc[janela['inicio_treino'] : janela['fim_treino']].dropna().copy()
        series_teste_original = series_completa.loc[janela['fim_treino'] : janela['fim_teste']].copy()

        # 2. Simula Gaps e Obt√©m Target Real
        # Simula_lacunas_seq_3 deve vir da C√©lula 2 ou 1
        series_teste_gaps, valores_reais = simula_lacunas_seq_3(series_teste_original.copy())

        if valores_reais.empty:
            print(f"--- INICIANDO {JANELA_FOCAL} (Treino: {len(series_treino):}h | Teste: 0 pontos de gap) ---")
            print(f"‚ö†Ô∏è Janela ignorada: N√£o h√° gaps v√°lidos com lag={LAG_ORDER} nesta janela.")
            continue

        # 3. Cria datasets de treino e teste com Lags
        X_train, y_train = cria_dataset_lags(series_treino, lag=LAG_ORDER)
        X_test_base, _ = cria_dataset_lags(series_teste_original, lag=LAG_ORDER)

        # Interse√ß√£o garante que X_test_final tem apenas os pontos onde o valor real √© conhecido E os lags existem.
        indices_validos = valores_reais.index.intersection(X_test_base.index)

        y_test_final = valores_reais.loc[indices_validos].copy()
        X_test_final = X_test_base.loc[indices_validos].copy()

        if y_test_final.empty or X_train.empty:
            print(f"--- INICIANDO {JANELA_FOCAL} (Treino: {len(series_treino):}h | Teste: {len(valores_reais)} pontos de gap) ---")
            print(f"‚ö†Ô∏è Janela ignorada: X_train ({len(X_train)}) ou X_test_final ({len(X_test_final)}) est√° vazio ap√≥s aplica√ß√£o de lag.")
            continue

        print(f"--- INICIANDO {JANELA_FOCAL} (Treino: {len(series_treino):}h | Teste: {len(y_test_final)} pontos de gap) ---")

        # --- Loop para MODELOS ---
        for nome_metodo, modelo_original in TODOS_METODOS.items():

            print(f"  -> EXECUTANDO: {nome_metodo}")

            # 4. Executa o Modelo (Treinamento e Predi√ß√£o)
            resultado_execucao = executa_modelo(
                nome_metodo,
                modelo_original,
                X_train,
                y_train,
                X_test_final,
                y_test_final
            )

            # 5. Processa o Resultado e Armazena
            if resultado_execucao is not None and isinstance(resultado_execucao, tuple) and len(resultado_execucao) == 2:

                metricas_dict, pred_array = resultado_execucao

                # CORRE√á√ÉO CR√çTICA: Assegura que estamos trabalhando com um dicion√°rio mut√°vel (c√≥pia)
                if isinstance(metricas_dict, dict):
                    # Clonamos o dicion√°rio retornado. Isso resolve o erro de 'tuple' object...
                    metricas_final = metricas_dict.copy()
                else:
                    print(f"      ‚ùå Erro cr√≠tico no loop do {nome_metodo}: Retorno de m√©tricas n√£o √© um dicion√°rio.")
                    continue

                # Adiciona Metadados (Esta era a parte que causava o erro se metricas_dict n√£o fosse copiado)
                metricas_final['Caracter√≠stica'] = caracteristica_focal
                metricas_final['Janela'] = JANELA_FOCAL
                metricas_final['Imputados (Horas)'] = len(y_test_final)

                # Armazena os resultados (M√©tricas)
                globals()['resultados_brutos'].append(metricas_final)

            else:
                # O loop continua, pulando o armazenamento de resultados para este modelo/janela
                pass

    # Libera√ß√£o de mem√≥ria entre caracter√≠sticas (para modelos grandes)
    gc.collect()


print("\n‚úÖ Processamento Rolling Origin conclu√≠do. Vari√°vel 'resultados_brutos' preenchida para o relat√≥rio.")

# C√âLULA 5: GERA√á√ÉO DO RELAT√ìRIO DE M√âTRICAS AGREGADAS
# --------------------------------------------------------------------------------

import pandas as pd
from tabulate import tabulate
import numpy as np

print("Iniciando a Gera√ß√£o do Relat√≥rio de M√©tricas Agregadas...")

try:
    if not globals().get('resultados_brutos'):
        raise ValueError("'resultados_brutos' n√£o encontrado ou est√° vazio. Execute a C√©lula 4 primeiro.")

    df_resultados = pd.DataFrame(globals()['resultados_brutos'])

    # 1. Agrega√ß√£o dos Resultados por Caracter√≠stica e M√©todo (M√©dia de Janelas)
    df_agregado = df_resultados.groupby(['Caracter√≠stica', 'M√©todo']).agg(
        **{
            'Janelas (Total)': ('Janela', 'nunique'),
            'Imputados (Horas)': ('Imputados (Horas)', 'sum'),
            'M√©dia MAE': ('MAE', 'mean'),
            'M√©dia MAPE': ('MAPE', 'mean'),
            'M√©dia MAD': ('MAD', 'mean'),
            'M√©dia RMSE': ('RMSE', 'mean')
        }
    ).reset_index()

    # Formata√ß√£o do Relat√≥rio Principal
    df_agregado_display = df_agregado.sort_values(by=['Caracter√≠stica', 'M√©dia MAE'])

    tabela_principal = tabulate(
        df_agregado_display,
        headers='keys',
        tablefmt='pipe',
        showindex=False,
        floatfmt=(".4f", ".4f", ".4f", ".4f", ".4f", ".4f")
    )

    print("\n========================================================================================================")
    print("                      RELAT√ìRIO DE M√âTRICAS AGREGADAS (M√âDIA POR JANELA)                      ")
    print("========================================================================================================")
    print(tabela_principal)

    print("\n--- AN√ÅLISE DE MELHOR DESEMPENHO (Menor MAE) ---")

    # 2. An√°lise de Melhor Desempenho (Menor MAE)
    for caracteristica in df_agregado['Caracter√≠stica'].unique():

        df_caracteristica = df_agregado[df_agregado['Caracter√≠stica'] == caracteristica].copy()

        # Filtra e ordena pelo menor MAE
        df_melhor_desempenho = df_caracteristica.sort_values(by='M√©dia MAE').reset_index(drop=True)

        # Seleciona as colunas para exibi√ß√£o
        df_melhor_desempenho_display = df_melhor_desempenho[[
            'M√©todo', 'M√©dia MAE', 'M√©dia RMSE', 'Janelas (Total)', 'Imputados (Horas)'
        ]]

        tabela_mae = tabulate(
            df_melhor_desempenho_display,
            headers='keys',
            tablefmt='pipe',
            showindex=False,
            floatfmt=(".4f", ".4f")
        )

        print(f"\nMelhor MAE para: {caracteristica}")
        print(tabela_mae)

except Exception as e:
    print(f"‚ùå Erro: {e}. Execute a C√©lula 4 primeiro.")

print("\n‚úÖ Relat√≥rio conclu√≠do e an√°lise de desempenho gerada.")

# C√âLULA FINAL 4.0: Plotagem Otimizada (Janela √önica + Carregamento Seguro)
# --------------------------------------------------------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import gc
# Imports de Machine Learning (apenas para o MAE)
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor

print("Iniciando a gera√ß√£o de gr√°ficos de zoom focados na Melhor Janela por Caracter√≠stica...")

# --- FUN√á√ÉO DE CARREGAMENTO DE DADOS (Assumindo que est√° definida em outro lugar) ---
def carrega_dados_csv_e_prepara():
    """
    Fun√ß√£o Placeholder para carregar os dados brutos e gerar o df_global,
    caso n√£o esteja na mem√≥ria.
    Ajuste este bloco para refletir sua fun√ß√£o real de carregamento e preparo.
    """
    try:
        # Tenta usar a fun√ß√£o global definida (exemplo do seu projeto)
        return globals()['carrega_dados_csv_e_prepara_global']()
    except (KeyError, NameError):
        print("üö® Tentando carregar dados. Se falhar, defina 'carrega_dados_csv_e_prepara' e execute uma c√©lula antes.")
        # Simula√ß√£o m√≠nima de carregamento (NECESSITA SER AJUSTADO POR VOC√ä)
        # Se for necess√°rio, descomente o c√≥digo que usa 'files.upload()' ou o seu pr√≥prio m√©todo de I/O.
        # Por exemplo: df = pd.read_csv('seu_arquivo.csv', index_col='data_hora', parse_dates=True)
        # √â VITAL que df_global seja um DataFrame com as colunas certas e √≠ndice de data.
        raise NameError("df_global ou a fun√ß√£o de carregamento n√£o foi encontrada. Execute as c√©lulas de SETUP e CARREGAMENTO de dados.")

# --- GARANTE O CARREGAMENTO DE DADOS E VARI√ÅVEIS CHAVE ---
if 'df_global' not in globals():
    df_global = carrega_dados_csv_e_prepara()

if 'janelas_validacao' not in globals() or 'LAG_ORDER' not in globals():
    print("üö® Aviso: Vari√°veis janelas_validacao e LAG_ORDER n√£o encontradas. Verifique as c√©lulas de SETUP.")
    # Sa√≠da segura
    exit()

# --- MAPA DE MELHOR DESEMPENHO (MAE M√≠nimo) POR CARACTER√çSTICA ---
# Utilizando janelas de tempo diferentes (0, 2, 4) para garantir per√≠odos de teste distintos.
MAPA_MELHOR_DESEMPENHO = {
    'PRECIPITACAO_HORARIOMM': {'metodo': 'LSTM Regressor', 'janela_idx': 0, 'mae': 0.0083}, # Ex: Janela 1
    'TEMPERATURA_AR_HORARIA': {'metodo': 'LSTM Regressor', 'janela_idx': 2, 'mae': 1.7088}, # Ex: Janela 3
    'UMIDADE_AR_HORARIA':     {'metodo': 'Random Forest', 'janela_idx': 4, 'mae': 8.1500}, # Ex: Janela 5
}

# --- CONFIGURA√á√ïES DE PLOTAGEM ---
CORES = {'Random Forest': '#1f77b4', 'KNN Regressor': '#2ca02c', 'GBR Regressor': '#ff7f0e', 'LSTM Regressor': '#d62728'}
ESTILOS = {'Random Forest': '-', 'KNN Regressor': ':', 'GBR Regressor': '--', 'LSTM Regressor': '-.'}
COR_LINHA_GUIA = 'lightgray'
LARGURA_LINHA_GUIA = 1.5
ZOOM_MARGIN_HOURS = 4

# --- LOOP PRINCIPAL: GERA UM GR√ÅFICO POR CARACTER√çSTICA ---

for caracteristica_focal, info_melhor in MAPA_MELHOR_DESEMPENHO.items():

    # 1. Defini√ß√µes de Foco
    janela_idx = info_melhor['janela_idx']
    janela_focal = globals()['janelas_validacao'][janela_idx]
    janela_nome = janela_focal['nome']
    LAG_ORDER = globals()['LAG_ORDER']

    print(f"\n################################################################################")
    print(f"PROCESSANDO CARACTER√çSTICA: {caracteristica_focal} (Foco: {janela_nome})")
    print(f"################################################################################")

    # Estrutura para armazenar as s√©ries completas
    series_imputadas = {}

    # Fatiamento da Janela Focal
    series_janela = globals()['df_global'][caracteristica_focal].copy()
    series_treino = series_janela.loc[janela_focal['inicio_treino'] : janela_focal['fim_treino']].iloc[:-1].copy()
    series_teste_original = series_janela.loc[janela_focal['fim_treino'] : janela_focal['fim_teste']].iloc[1:].copy()

    # Simula√ß√£o de gaps
    # Assumimos que simula_lacunas_seq_3 foi definida em uma c√©lula anterior
    series_teste_gaps, valores_reais = globals()['simula_lacunas_seq_3'](series_teste_original.dropna().copy())

    # Adiciona a s√©rie REAL/Original para plotagem (Treino + Teste)
    series_imputadas[caracteristica_focal + ' (Original)'] = globals()['df_global'][caracteristica_focal].loc[janela_focal['inicio_treino'] : janela_focal['fim_teste']].copy()

    # --- 2. PREPARA√á√ÉO DE DADOS PARA CALCULAR O √çNDICE CORRETO DO GAP (LAG AWARE) ---
    # Assumimos que cria_dataset_lags foi definida em uma c√©lula anterior
    X_test_base, _ = globals()['cria_dataset_lags'](series_teste_original, lag=LAG_ORDER)
    indices_validos = valores_reais.index.intersection(X_test_base.index)
    X_test_final = X_test_base.loc[indices_validos].copy()

    if X_test_final.empty:
        print(f"  ‚ùå N√£o h√° pontos v√°lidos para previs√£o na {janela_nome} (ap√≥s lags). Pulando.")
        continue


    # --- 3. EXECU√á√ÉO DO FLUXO DE IMPUTA√á√ÉO (REQUER SETUP CORRETO) ---
    mae_list = []

    for nome_metodo, funcao_metodo in globals()['TODOS_METODOS'].items():
        try:
            # Esta linha REQUER que o setup de TODOS_METODOS esteja correto.
            imputacoes, contagem, info = funcao_metodo(series_treino.copy(), series_teste_gaps.copy(), valores_reais)

            if imputacoes is not None and contagem > 0:
                df_imput = pd.DataFrame(imputacoes)
                df_imput.set_index('data_hora', inplace=True)
                pred_array = df_imput['imputado']

                # RE-CALCULA o MAE para refer√™ncia
                pred_mae = pred_array.loc[X_test_final.index].values
                real_mae = valores_reais.loc[X_test_final.index].values
                mae = np.mean(np.abs(pred_mae - real_mae))
                mae_list.append(f"{nome_metodo} (MAE: {mae:.4f})")

                # Armazena a s√©rie imputada
                serie_imputada = series_teste_gaps.copy()
                serie_imputada.loc[pred_array.index] = pred_array

                series_imputadas[nome_metodo] = serie_imputada

        except TypeError as e:
            print(f"  ‚ö†Ô∏è ERRO CR√çTICO no setup para {nome_metodo}. (Falha ao chamar a fun√ß√£o)")
            pass # Continua para os pr√≥ximos m√©todos mesmo com erro
        except Exception as e:
            print(f"  ‚ùå Erro FATAL na execu√ß√£o de {nome_metodo}: {e}")
            pass

    # --- 4. PREPARA√á√ÉO E GERA√á√ÉO DO GR√ÅFICO ---
    df_plot = pd.DataFrame(series_imputadas)
    df_plot['Gaps (Simulado)'] = series_teste_gaps

    fig, ax = plt.subplots(figsize=(15, 6))

    # Destaque do Melhor Modelo (Caixa de Texto)
    melhor_metodo = info_melhor['metodo']
    melhor_mae = info_melhor['mae']
    ax.text(0.01, 0.95,
             f"MELHOR MOMENTO:\nModelo: {melhor_metodo}\nMAE: {melhor_mae:.4f} (Reportado)",
             transform=ax.transAxes, fontsize=10, verticalalignment='top',
             bbox=dict(boxstyle="round,pad=0.5", fc=CORES.get(melhor_metodo, 'lightgray'), alpha=0.6))

    # S√©rie Original (LINHA GUIA - Cinza claro e fina)
    df_plot[caracteristica_focal + ' (Original)'].plot(
        ax=ax, label='Valor Real (Original)', color=COR_LINHA_GUIA, linewidth=LARGURA_LINHA_GUIA,
        alpha=0.9, linestyle='-', zorder=1, marker='o', markersize=3
    )

    # Plotar as S√©ries Imputadas (com linha) - DESTAQUE
    for metodo in [m for m in globals()['TODOS_METODOS'].keys()]:
        if metodo in df_plot.columns:
            df_plot[metodo].plot(
                ax=ax, label=f'Imputa√ß√£o ({metodo})',
                color=CORES.get(metodo, 'black'),
                linewidth=2,
                linestyle=ESTILOS.get(metodo, '-'),
                zorder=3
            )

    # Destaque do Per√≠odo do Gap (Zona Cinza)
    if not X_test_final.empty:
        # Usar o √≠ndice da √°rea de previs√£o (X_test_final) para definir o gap
        data_inicio_gap = X_test_final.index.min()
        data_fim_gap = X_test_final.index.max()

        # Destacar a √°rea do Gap (Zona Cinza)
        ax.axvspan(data_inicio_gap, data_fim_gap, color='gray', alpha=0.2, label='Per√≠odo de Gap', zorder=0)

        # Aplicar Zoom FOCADO NA LACUNA (CORRE√á√ÉO)
        zoom_inicio = data_inicio_gap - pd.Timedelta(hours=ZOOM_MARGIN_HOURS)
        zoom_fim = data_fim_gap + pd.Timedelta(hours=ZOOM_MARGIN_HOURS)
        ax.set_xlim(zoom_inicio, zoom_fim)

        # Define o limite de valor (Zoom no eixo Y)
        cols_para_zoom = [c for c in df_plot.columns if 'Original' in c or c in globals()['TODOS_METODOS'].keys()]
        valores_zoom = df_plot.loc[zoom_inicio:zoom_fim, cols_para_zoom].values.flatten()
        valores_zoom = valores_zoom[~np.isnan(valores_zoom)]

        if valores_zoom.size > 0:
            min_y = valores_zoom.min()
            max_y = valores_zoom.max()
            margin_y = (max_y - min_y) * 0.1

            if caracteristica_focal == 'PRECIPITACAO_HORARIOMM':
                 margin_y = max(0.05, margin_y)
                 ax.set_ylim(max(0, min_y - margin_y), max_y + margin_y)
            else:
                 ax.set_ylim(min_y - margin_y, max_y + margin_y)

    # Configura√ß√µes Finais do Gr√°fico
    ax.set_title(f'Gr√°fico de Zoom Comparativo: {caracteristica_focal} (Melhor Desempenho na {janela_nome})\n' +
                 f"M√©tricas (MAE re-calculado): {', '.join(mae_list)}", fontsize=14)
    ax.set_xlabel('Data e Hora', fontsize=12)
    ax.set_ylabel(f'{caracteristica_focal}', fontsize=12)
    ax.legend(loc='lower right', fontsize=10)
    ax.grid(True, alpha=0.5)

    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %Hh'))
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
    plt.close(fig)
    gc.collect()

print("\n‚úÖ Gera√ß√£o de todos os gr√°ficos de zoom conclu√≠da (Zoom focado na lacuna e setup de dados corrigido).")